{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import nltk \n",
    "\n",
    "\n",
    "file_path = 'path/to/your/en_df_senza_libro_extra.json'\n",
    "\n",
    "# Load the JSON file into a pandas DataFrame\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Use nltk's word_tokenize for tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    nlp_it = spacy.load('it_core_news_sm')\n",
    "    nlp_es = spacy.load('es_core_news_sm')\n",
    "    \n",
    "    #remove footnotes\n",
    "    footnote_pattern = r'\\d+\\s+[A-Z]\\.\\s+[A-Z][a-z]+,?\\s+“[^”]+”,\\s+cit\\.?,\\s+(p\\.|pp\\.)\\s+\\d+(-\\d+)?\\.?'\n",
    "    text = re.sub(footnote_pattern, '', text)\n",
    "   \n",
    "    # Define a regular expression pattern for websites\n",
    "    website_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Defining a regular expression pattern for emails\n",
    "    email_pattern = r'\\S+@\\S+'\n",
    "    \n",
    "    # Defining a regular expression pattern for phone numbers\n",
    "    phone_number_pattern = r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    \n",
    "     # removing words connected to numbers\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    \n",
    "    # Words Connected to Numbers with Specific Punctuation\n",
    "    text = re.sub(r'\\b\\w+[-/]\\d+|\\d+[-/]\\w+\\b', '', text)\n",
    "    \n",
    "    #removing words connected to punctuation\n",
    "    text = re.sub(r'\\w*[\\d,.!?;:]+\\w*', '', text)\n",
    "\n",
    "\n",
    "    # removing websites, emails, and phone numbers\n",
    "    text = re.sub(website_pattern, '', text)\n",
    "    text = re.sub(email_pattern, '', text)\n",
    "    text = re.sub(phone_number_pattern, '', text)\n",
    "   \n",
    "    \n",
    "    # removing numeric values\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "   \n",
    "    # removing HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # removing all occurrences of 'º'\n",
    "    text = re.sub(r'º+', '', text) \n",
    "    \n",
    "    # removing Roman numerals\n",
    "    text = re.sub(r'\\b(?:i{1,3}|iv|v|vi{1,3}|ix|x|xi{1,3}|xl|l|lx|xc|c|cc|ccc|cd|d|dc|dcc|dccc|cm|m|mm|mmm)\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # removing words with three same characters in sequence\n",
    "    text = re.sub(r'\\b\\w*(\\w)\\1{2,}\\w*\\b', '', text)\n",
    "    \n",
    "    \n",
    "    #removing punctuations\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Using custom tokenizer for better tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]  # Exclude non-alphabetic tokens\n",
    "    \n",
    "    # removing words with accents by filtering out tokens that are not ASCII\n",
    "    tokens = [token for token in tokens if all(ord(char) < 128 for char in token)]\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english')) | nlp_it.Defaults.stop_words | nlp_es.Defaults.stop_words\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    \n",
    "    # removing specific words\n",
    "    exclusion_list = ['—', ',', '–', '’,', '``', 'et','eg', 'al']\n",
    "    tokens = [word for word in tokens if word not in exclusion_list and len(word) > 1]\n",
    "\n",
    "    \n",
    "    # Lemmatizing the tokens\n",
    "    \n",
    "    doc = nlp(' '.join(tokens))\n",
    "    tokens_final = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    bigrams = list(ngrams(tokens_final, 2))\n",
    "    named_entities = [(entity.text.lower(), entity.label_) for entity in doc.ents]\n",
    "    named_entity_bigrams = [(bigram, label) for bigram in bigrams for word, label in named_entities if word.lower() in bigram]\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    return named_entity_bigrams #Or return named entities to get a seperate list of entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing each document seperately to examine the output\n",
    "\n",
    "text_to_preprocess = en_df.iloc[24, 1] #defining the row and column containing the doc\n",
    "twenty_fifth_row = preprocess_text(text_to_preprocess)\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(twenty_fifth_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining top 100 frequent words and entities and check for duplicates\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "tks = twenty_fifth_row\n",
    "\n",
    "# Creating a Counter for word counts\n",
    "word_counter = Counter(tks)\n",
    "\n",
    "# Get the 10 most common words\n",
    "most_common_words = word_counter.most_common(100)\n",
    "\n",
    "# Check for duplicates in the top 100 most frequent words\n",
    "most_common_duplicates = [term for term, count in Counter(most_common_terms).items() if count > 1]\n",
    "if most_common_duplicates:\n",
    "    print(\"Duplicate terms in the top 100 most frequent words:\", most_common_duplicates)\n",
    "else:\n",
    "    print(\"There are no duplicate terms in the top 100 most frequent words.\")\n",
    "\n",
    "\n",
    "print(\"Top 100 most frequent words:\")\n",
    "for word, count in most_common_words:\n",
    "    token = nlp(word)\n",
    "    named_entity = token.ents[0].label_ if token.ents else 'None'\n",
    "    print(f\"{word}: {count} , {named_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "\n",
    "def summarize_text(text, sentences_count=3):\n",
    "\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count)\n",
    "    summarized_text = \" \".join(str(sentence) for sentence in summary)\n",
    "    \n",
    "    return summarized_text\n",
    "\n",
    "df['summarized_text'] = df['original_text_partial'].apply(summarize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to json\n",
    "\n",
    "file_path = '/mnt/data/en_df_senza_libro_extra.json'\n",
    "\n",
    "# Save the JSON data to a file\n",
    "df.to_json(file_path, orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
