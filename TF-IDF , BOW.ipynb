{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb10670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Replace the path with the actual path to your JSON file\n",
    "file_path = '/content/drive/MyDrive/combined_texts_preprocessed.json'\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "combined_texts_preprocessed = pd.read_json(file_path, lines=True)\n",
    "\n",
    "combined_texts_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd606d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten bigrams correctly without separating them\n",
    "def flatten_bigrams(bigrams):\n",
    "    # Flatten the list of lists into a single list of bigram strings\n",
    "    return [f\"{bigram[0]}_{bigram[1]}\" for bigram in bigrams]\n",
    "\n",
    "# Apply the flatten function to each document\n",
    "combined_texts_preprocessed['flattened_bigrams'] = combined_texts_preprocessed['preprocessed_text_bigrams'].apply(flatten_bigrams)\n",
    "\n",
    "# Combine unigrams and flattened bigrams\n",
    "def combine_tokens(unigrams, bigrams):\n",
    "    return unigrams + bigrams\n",
    "\n",
    "combined_texts_preprocessed['aggregated_tokens'] = combined_texts_preprocessed.apply(\n",
    "    lambda row: combine_tokens(row['preprocessed_text'], row['flattened_bigrams']), axis=1\n",
    ")\n",
    "\n",
    "# Step 2: Create a Gensim dictionary from the aggregated tokens\n",
    "dictionary = corpora.Dictionary(combined_texts_preprocessed['aggregated_tokens'])\n",
    "\n",
    "# Filter out tokens with a document frequency of 1 (singleton or very rare terms)\n",
    "low_df_ids = [token_id for token_id, docfreq in dictionary.dfs.items() if docfreq <= 1]\n",
    "dictionary.filter_tokens(bad_ids=low_df_ids)\n",
    "\n",
    "# Update the corpus to exclude the filtered tokens\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in combined_texts_preprocessed['aggregated_tokens']]\n",
    "\n",
    "# Step 3: Generate the TF-IDF matrix\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "# Now you have `dictionary`, `bow_corpus`, and `tfidf_corpus` available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c53b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to train LDA model\n",
    "def train_lda_model(corpus, dictionary, num_topics, chunksize=2000, passes=20, iterations=400, eval_every=None):\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Function to compute coherence score\n",
    "def compute_coherence_values(model, corpus, dictionary, texts, coherence='c_v'):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, corpus=corpus, coherence=coherence)\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "# List to store coherence values\n",
    "coherence_values_cv = []\n",
    "coherence_values_umass = []\n",
    "models = []\n",
    "\n",
    "# Range of topics to try\n",
    "topic_range = range(2, 11)  # Example: topics from 2 to 10\n",
    "\n",
    "for num_topics in topic_range:\n",
    "    print(f\"Training LDA model with {num_topics} topics...\")\n",
    "    \n",
    "    # Train LDA model\n",
    "    model = train_lda_model(bow_corpus, dictionary, num_topics)\n",
    "    models.append(model)\n",
    "    \n",
    "    # Compute coherence values\n",
    "    coherence_cv = compute_coherence_values(model, bow_corpus, dictionary, combined_texts_preprocessed['aggregated_tokens'], coherence='c_v')\n",
    "    coherence_umass = compute_coherence_values(model, bow_corpus, dictionary, combined_texts_preprocessed['aggregated_tokens'], coherence='u_mass')\n",
    "    \n",
    "    coherence_values_cv.append(coherence_cv)\n",
    "    coherence_values_umass.append(coherence_umass)\n",
    "    \n",
    "    print(f\"Coherence (c_v): {coherence_cv}, Coherence (u_mass): {coherence_umass}\")\n",
    "\n",
    "# Plot coherence scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot c_v\n",
    "plt.plot(topic_range, coherence_values_cv, label=\"c_v Coherence\", marker='o')\n",
    "\n",
    "# Plot UMass\n",
    "plt.plot(topic_range, coherence_values_umass, label=\"u_mass Coherence\", marker='o')\n",
    "\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend((\"c_v Coherence\", \"u_mass Coherence\"), loc='best')\n",
    "plt.title(\"Coherence Scores by Number of Topics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67df1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 3\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58364d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(bow_corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd763d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 words and their weights for each topic\n",
    "num_words = 5\n",
    "top_words_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    top_words = model.show_topic(i, topn=num_words)\n",
    "    top_words_per_topic.append(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe08587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 words and their weights for each topic\n",
    "num_words = 5\n",
    "top_words_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    top_words = model.show_topic(i, topn=num_words)\n",
    "    top_words_per_topic.append(top_words)\n",
    "\n",
    "def plot_topic_barcharts(top_words_per_topic):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 16))  # 2x2 grid for 4 topics\n",
    "    axs = axs.flatten()  # Flatten the array to easily iterate over it\n",
    "\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c',]  # Colors for the bars\n",
    "\n",
    "    for topic_id, topic in enumerate(top_words_per_topic):\n",
    "        words, weights = zip(*topic)\n",
    "\n",
    "        # Calculate word counts from bow_corpus\n",
    "        word_counts = []\n",
    "        for word in words:\n",
    "            word_id = dictionary.token2id[word]\n",
    "            count = sum([count for doc in bow_corpus for word_id_in_doc, count in doc if word_id_in_doc == word_id])\n",
    "            word_counts.append(count)\n",
    "\n",
    "        ax_count = axs[topic_id].twinx()  # Create a twin axes sharing the x-axis\n",
    "\n",
    "        # Bar chart for word counts\n",
    "        axs[topic_id].bar(np.arange(len(words)), word_counts, color=colors[topic_id], alpha=0.9,width=0.2, label='Word Count')\n",
    "        axs[topic_id].set_ylabel('Word Count', color=colors[topic_id])\n",
    "        axs[topic_id].tick_params(axis='y', labelcolor=colors[topic_id])\n",
    "\n",
    "        # Bar chart for weights\n",
    "        ax_count.bar(np.arange(len(words)) + 0.4, weights, color=colors[topic_id], alpha=0.4, width=0.3, label='Weights')\n",
    "        ax_count.set_ylabel('Weights', color=colors[topic_id])\n",
    "        ax_count.tick_params(axis='y', labelcolor=colors[topic_id])\n",
    "\n",
    "        axs[topic_id].set_xticks(np.arange(len(words)) + 0.2)\n",
    "        axs[topic_id].set_xticklabels(words, rotation=45, ha='right')\n",
    "        axs[topic_id].set_title(f'Topic {topic_id}', fontsize=16, color=colors[topic_id])\n",
    "\n",
    "        axs[topic_id].legend(loc='upper left')\n",
    "        ax_count.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the bar charts\n",
    "plot_topic_barcharts(top_words_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic distribution over campo_analisi\n",
    "\n",
    "\n",
    "# Step 1: Get the topic distribution for each document (this part of the code remains the same)\n",
    "topic_distributions = []\n",
    "for doc_bow in bow_corpus:\n",
    "    # Get the topic distribution for the document\n",
    "    doc_topics = model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "    # Convert to an array and normalize the probabilities\n",
    "    doc_topics_array = np.array([topic_prob for _, topic_prob in doc_topics])\n",
    "\n",
    "    # Set contributions less than 1% to zero\n",
    "    doc_topics_array[doc_topics_array < 0.01] = 0\n",
    "\n",
    "    # Append the results\n",
    "    topic_distributions.append(doc_topics_array)\n",
    "\n",
    "# Step 2: Create a DataFrame from the topic distributions.\n",
    "df_topic_distribution = pd.DataFrame(topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
    "\n",
    "# Step 3: Assuming df is your original dataframe with a 'Campo_analisi' column that links documents to fields\n",
    "df_topic_distribution['Campo_analisi'] = combined_texts_preprocessed['Campo_analisi']\n",
    "\n",
    "# Step 4: Aggregate topic distributions by field (Campo_analisi)\n",
    "df_field_distribution = df_topic_distribution.groupby('Campo_analisi').mean()\n",
    "\n",
    "# Step 5: Find the dominant topic for each field (if needed)\n",
    "df_field_distribution['Dominant Topic'] = df_field_distribution.idxmax(axis=1)\n",
    "\n",
    "# Step 6: Display the final table with topic distributions over fields\n",
    "df_field_distribution.reset_index(inplace=True)\n",
    "\n",
    "# Display the final table\n",
    "df_field_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Step 1: Filter out only numeric topic distribution columns\n",
    "# Make sure you're excluding any columns that do not represent topics (like 'Campo_analisi' and 'Dominant Topic')\n",
    "topic_columns = [col for col in df_field_distribution.columns if col.startswith('Topic')]\n",
    "df_topic_only = df_field_distribution[topic_columns].copy()\n",
    "\n",
    "# Step 2: Compute cosine similarity between the fields\n",
    "cosine_sim_matrix = cosine_similarity(df_topic_only)\n",
    "\n",
    "# Step 3: Convert the similarity matrix to a DataFrame for better readability\n",
    "df_cosine_similarity = pd.DataFrame(cosine_sim_matrix, index=df_field_distribution['Campo_analisi'], columns=df_field_distribution['Campo_analisi'])\n",
    "\n",
    "# Display the cosine similarity matrix\n",
    "df_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to compute the BERT embeddings for a text\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token's representation as the sentence embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Assuming you have 'combined_texts_preprocessed' DataFrame with 'text' and 'Campo_analisi' columns\n",
    "# Compute the mean embeddings for each 'Campo_analisi'\n",
    "combined_texts_preprocessed['bert_embedding'] = combined_texts_preprocessed['original_text_partial'].apply(get_bert_embedding)\n",
    "campo_analisi_embeddings = combined_texts_preprocessed.groupby('Campo_analisi')['bert_embedding'].apply(lambda x: x.mean(axis=0))\n",
    "\n",
    "# Convert to a DataFrame for cosine similarity computation\n",
    "campo_analisi_embeddings_df = pd.DataFrame(campo_analisi_embeddings.tolist(), index=campo_analisi_embeddings.index)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim_matrix = cosine_similarity(campo_analisi_embeddings_df)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=campo_analisi_embeddings.index, columns=campo_analisi_embeddings.index)\n",
    "\n",
    "# Display the cosine similarity matrix\n",
    "cosine_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f00ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Step 1: Normalize the embeddings to make them similar to probability distributions\n",
    "def normalize_embeddings(embedding):\n",
    "    # Ensure all elements are non-negative and normalize\n",
    "    embedding = np.abs(embedding)  # Make sure embeddings are non-negative\n",
    "    norm_embedding = embedding / np.linalg.norm(embedding, ord=1)  # Normalize by L1 norm (sum)\n",
    "    return norm_embedding\n",
    "\n",
    "# Step 2: Function to compute Hellinger distance between two probability distributions\n",
    "def hellinger_distance(p, q):\n",
    "    return np.sqrt(0.5 * np.sum((np.sqrt(p) - np.sqrt(q)) ** 2))\n",
    "\n",
    "# Step 3: Normalize embeddings for each campo_analisi\n",
    "campo_analisi_normalized_embeddings = campo_analisi_embeddings.apply(normalize_embeddings)\n",
    "\n",
    "# Convert to a DataFrame for Hellinger distance computation\n",
    "campo_analisi_normalized_embeddings_df = pd.DataFrame(campo_analisi_normalized_embeddings.tolist(), index=campo_analisi_normalized_embeddings.index)\n",
    "\n",
    "# Step 4: Compute pairwise Hellinger distances between each campo_analisi\n",
    "pairwise_hellinger_distances = pdist(campo_analisi_normalized_embeddings_df.values, metric=hellinger_distance)\n",
    "\n",
    "# Convert the distances into a square matrix for better readability\n",
    "hellinger_distance_matrix = squareform(pairwise_hellinger_distances)\n",
    "\n",
    "# Step 5: Convert to DataFrame for readability\n",
    "hellinger_distance_df = pd.DataFrame(hellinger_distance_matrix, index=campo_analisi_normalized_embeddings.index, columns=campo_analisi_normalized_embeddings.index)\n",
    "\n",
    "# Display the Hellinger distance matrix\n",
    "print(\"Mean Pairwise Hellinger Distance Between Fields:\")\n",
    "hellinger_distance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fae148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the Hellinger distance matrix from 1 to get a similarity matrix\n",
    "hellinger_similarity_matrix = 1 - hellinger_distance_df\n",
    "\n",
    "# Display the Hellinger similarity matrix\n",
    "print(\"Pairwise Similarity Between Fields (1 - Hellinger Distance):\")\n",
    "hellinger_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8572b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all matrices have the same shape\n",
    "if cosine_sim_df.shape == hellinger_similarity_matrix.shape == df_cosine_similarity.shape:\n",
    "\n",
    "    # Convert DataFrames to NumPy arrays for computation\n",
    "    cosine_sim_array = cosine_sim_df.values\n",
    "    hellinger_similarity_matrix_array = hellinger_similarity_matrix.values\n",
    "    df_cosine_similarity_array = df_cosine_similarity.values\n",
    "\n",
    "    # Compute the element-wise mean of the three matrices\n",
    "    mean_matrix_array = (cosine_sim_array + hellinger_similarity_matrix_array + df_cosine_similarity_array) / 3\n",
    "\n",
    "    # Convert back to DataFrame (optional for better readability)\n",
    "    mean_matrix_df = pd.DataFrame(mean_matrix_array, index=cosine_sim_df.index, columns=cosine_sim_df.columns)\n",
    "\n",
    "    # Display the resulting mean matrix\n",
    "    print(\"Mean Matrix:\")\n",
    "mean_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for very small values close to zero\n",
    "threshold = 1e-7\n",
    "\n",
    "# Subtract from 1 to convert similarity to distance\n",
    "distance_matrix_array = 1 - mean_matrix_array\n",
    "\n",
    "# Correct the diagonal to be exactly zero\n",
    "np.fill_diagonal(distance_matrix_array, 0)\n",
    "\n",
    "# Set very small negative or near-zero values to 0\n",
    "distance_matrix_array[distance_matrix_array < threshold] = 0\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix_array, index=mean_matrix_df.index, columns=mean_matrix_df.columns)\n",
    "\n",
    "# Display the corrected distance matrix\n",
    "print(\"Corrected Distance Matrix:\")\n",
    "distance_matrix_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db034242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for very small values close to zero\n",
    "threshold = 1e-7\n",
    "\n",
    "# Subtract from 1 to convert similarity to distance\n",
    "distance_matrix_array = 1 - mean_matrix_array\n",
    "\n",
    "# Correct the diagonal to be exactly zero\n",
    "np.fill_diagonal(distance_matrix_array, 0)\n",
    "\n",
    "# Set very small negative or near-zero values to 0\n",
    "distance_matrix_array[distance_matrix_array < threshold] = 0\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix_array, index=mean_matrix_df.index, columns=mean_matrix_df.columns)\n",
    "\n",
    "# Display the corrected distance matrix\n",
    "print(\"Corrected Distance Matrix:\")\n",
    "distance_matrix_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "num_clusters = 3  # Choose the appropriate number of clusters\n",
    "cluster_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Set the diagonal of the distance matrix to 0\n",
    "np.fill_diagonal(distance_matrix_df.values, 0)\n",
    "\n",
    "# Now, calculate the Silhouette Score\n",
    "silhouette_avg = silhouette_score(distance_matrix_df, cluster_labels, metric='precomputed')\n",
    "\n",
    "# Print the Silhouette Score\n",
    "print(f'Silhouette Score for {num_clusters} clusters: {silhouette_avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8454ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert DataFrames to NumPy arrays\n",
    "hellinger_array = hellinger_similarity_matrix.values\n",
    "df_cosine_array = df_cosine_similarity.values\n",
    "cosine_array = cosine_sim_df.values\n",
    "mean_array = mean_matrix_df.values\n",
    "\n",
    "# Get the shape of the matrices (assuming they all have the same shape)\n",
    "rows, cols = hellinger_array.shape\n",
    "\n",
    "# Create an empty array to store the combined results\n",
    "combined_matrix = np.empty((rows, cols, 2), dtype=object)  # Use dtype=object to store tuples\n",
    "\n",
    "# Fill the combined matrix with values from the original matrices\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        combined_matrix[i, j, 0] = (hellinger_array[i, j], df_cosine_array[i, j], cosine_array[i, j])  # First row values\n",
    "        combined_matrix[i, j, 1] = mean_array[i, j]  # Second row value\n",
    "\n",
    "# Convert combined_matrix to a DataFrame for better readability\n",
    "combined_df = pd.DataFrame(index=mean_matrix_df.index, columns=mean_matrix_df.columns)\n",
    "\n",
    "# Populate the DataFrame with tuples (first row values, second row values)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if i == j:  # Check for diagonal\n",
    "            combined_df.iat[i, j] = ('', '')  # Set diagonal to '-'\n",
    "        else:\n",
    "            combined_df.iat[i, j] = (\n",
    "                (round(hellinger_array[i, j], 2), round(df_cosine_array[i, j], 2), round(cosine_array[i, j], 2)),\n",
    "                round(mean_array[i, j], 2)\n",
    "            )  # Round values to 2 decimal points\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
